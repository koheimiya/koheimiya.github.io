## Bio
* Researcher (applied mathematics, machine learning)
* Working at IBM Research - Tokyo

## Publication
* Miyaguchi, K. & Yamanishi, K. (2019). Adaptive Minimax Regret against Smooth Logarithmic Losses over High-Dimensional l1-Balls via Envelope Complexity. In Proceedings of the 22nd International Conference on  Artificial Intelligence and Statistics (AISTATS'19), 89, 3440-3448, Okinawa, Japan.
* Miyaguchi, K. & Kajino, K. (2019). Cogra: Concept-drift-aware Stochastic Gradient Descent for Time-series Forecasting, In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19), xx-xx, Hawaii, USA.
* Miyaguchi, K., & Yamanishi, K. (2018). High-dimensional penalty selection via minimum description length principle. Machine Learning, 107(8-10), 1283-1302.
* Miyaguchi, K., Matsushima, S., & Yamanishi, K. (2017). Sparse Graphical Modeling via Stochastic Complexity. In Proceedings of 2017 SIAM International Conference on Data Mining, 723-731, Texas, USA.
* Miyaguchi, K., Matsushima, S., & Yamanishi, K. (2016). Stochastic Complexity for Sparse Modeling. In Proceedings of Ninth Workshop on Information Theoretic Methods in Science and Engineering, 24-25, Helsinki, Finland.
* Miyaguchi, K., & Yamanishi, K. (2015). On-line detection of continuous changes in stochastic processes. In Proceedings of IEEE International Conference on Data Science and Advanced Analytics, 1-9.

## Code
* [Graphical lasso](https://github.com/koheimiya/pyglassobind) (implemented with C++, binded to Python via pybind11)
* Others: [GitHub](https://github.com/koheimiya), [Bitbucket](https://bitbucket.org/kmiya/)

## Blog

## Contact
(firstname)(lastname)(at)gmail(dot)com // No space in between
